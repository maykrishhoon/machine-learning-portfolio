{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŽ“ Indikraft Machine Learning Internship: Final Portfolio\n",
        "**Intern Name:** KRISH\n",
        "\n",
        "**Role:** Machine Learning Intern\n",
        "\n",
        "**Date:** 05/12/2025\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“˜ Executive Summary\n",
        "This notebook represents the culmination of my internship at Indikraft. Over the course of this program, I have successfully designed, built, and deployed **5 Machine Learning & Data Science projects**, ranging from Natural Language Processing (NLP) to Regression Analysis and Unsupervised Clustering.\n",
        "\n",
        "### ðŸ› ï¸ Technical Toolkit\n",
        "Throughout these projects, I utilized the following technologies:\n",
        "* **Python:** Core programming logic.\n",
        "* **Scikit-Learn:** Model training (Regression, K-Means).\n",
        "* **Pandas & NumPy:** Data manipulation and feature engineering.\n",
        "* **Seaborn & Matplotlib:** Advanced data visualization.\n",
        "* **TextBlob:** Sentiment analysis and NLP."
      ],
      "metadata": {
        "id": "La-wwXKlBe4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy scikit-learn matplotlib seaborn textblob\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "print(\"âœ… Environment Setup Complete. All libraries loaded.\")"
      ],
      "metadata": {
        "id": "9HWRYn7dGeuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸš€ Project 1: Product Rating Predictor (NLP + Regression)\n",
        "\n",
        "### ðŸ“Œ Problem Statement\n",
        "E-commerce platforms receive thousands of reviews daily. Manually reading them to assign a star rating is impossible. The goal of this project was to build an AI that can **read text** and **predict a numeric rating (1-5)**.\n",
        "\n",
        "### âš™ï¸ Methodology\n",
        "1.  **Data Ingestion:** Created a dataset of customer reviews.\n",
        "2.  **Vectorization (TF-IDF):** Computers cannot understand words. I used *Term Frequency-Inverse Document Frequency* to convert text into mathematical vectors.\n",
        "3.  **Model Training:** Trained a Linear Regression model to find the correlation between specific words (e.g., \"Good\", \"Bad\") and the rating score."
      ],
      "metadata": {
        "id": "BvuQaAN4Dkt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_rating = {\n",
        "    'review': [\n",
        "        \"I absolutely love this product, it is amazing!\",\n",
        "        \"Terrible, waste of money and time.\",\n",
        "        \"It's okay, average quality but works.\",\n",
        "        \"Excellent performance and fast shipping.\",\n",
        "        \"Broken on arrival, very disappointed.\",\n",
        "        \"Good value for the price.\"\n",
        "    ],\n",
        "    'rating': [5, 1, 3, 5, 1, 4]\n",
        "}\n",
        "df_rating = pd.DataFrame(data_rating)\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n",
        "X = vectorizer.fit_transform(df_rating['review'])\n",
        "y = df_rating['rating']\n",
        "\n",
        "model_rating = LinearRegression()\n",
        "model_rating.fit(X, y)\n",
        "\n",
        "print(\"--- ðŸ¤– Model Prediction Demo ---\")\n",
        "test_reviews = [\"The quality is amazing\", \"This is garbage\"]\n",
        "test_vec = vectorizer.transform(test_reviews)\n",
        "predictions = model_rating.predict(test_vec)\n",
        "\n",
        "for review, score in zip(test_reviews, predictions):\n",
        "    print(f\"Review: '{review}'  -->  Predicted Rating: {np.clip(score, 1, 5):.1f} / 5.0\")"
      ],
      "metadata": {
        "id": "1HykNt6LGy5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ  Project 2: Price Predictor\n",
        "\n",
        "### ðŸ“Œ Problem Statement\n",
        "Predicting property prices is complex because raw data is often not \"model-ready.\" A computer sees the year \"1990\" as a number, not an age.\n",
        "\n",
        "### âš™ï¸ Feature Engineering Approach\n",
        "This project focused on **Feature Engineering**â€”the art of creating new data from existing data.\n",
        "* **Transformation:** Converted `Year_Built` â†’ `House_Age` (2025 - Year).\n",
        "* **Encoding:** Converted categorical locations ('City', 'Rural') into binary inputs (0s and 1s) using One-Hot Encoding."
      ],
      "metadata": {
        "id": "tUTwoctfDuMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_price = {\n",
        "    'Square_Feet': [850, 1200, 2500, 1800, 900, 3200],\n",
        "    'Year_Built':  [1990, 2015, 2020, 2005, 1980, 2022],\n",
        "    'Location':    ['Rural', 'Suburb', 'City', 'Suburb', 'Rural', 'City'],\n",
        "    'Price':       [120000, 250000, 550000, 320000, 110000, 750000]\n",
        "}\n",
        "df_price = pd.DataFrame(data_price)\n",
        "\n",
        "df_price['House_Age'] = 2025 - df_price['Year_Built']\n",
        "print(f\"âœ… Created 'House_Age' feature. Example: Built 1990 -> {df_price['House_Age'].iloc[0]} years old.\")\n",
        "\n",
        "df_final = pd.get_dummies(df_price, columns=['Location'])\n",
        "\n",
        "X_price = df_final.drop(['Price', 'Year_Built'], axis=1)\n",
        "y_price = df_final['Price']\n",
        "\n",
        "model_price = LinearRegression()\n",
        "model_price.fit(X_price, y_price)\n",
        "\n",
        "print(f\"âœ… Model Trained. R2 Score: {model_price.score(X_price, y_price):.4f}\")"
      ],
      "metadata": {
        "id": "dVyFOQJ_HLx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ›’ Project 3: Product Clustering (Unsupervised Learning)\n",
        "\n",
        "### ðŸ“Œ Problem Statement\n",
        "In recommendation systems, we often don't have labeled data (we don't know which products are \"Premium\" vs \"Budget\"). We need the AI to discover these groups automatically.\n",
        "\n",
        "### âš™ï¸ Methodology\n",
        "I utilized **K-Means Clustering**, an unsupervised algorithm.\n",
        "1.  **Scaling:** Standardized Price ($2000) and Review (4.5) to the same scale so Price didn't dominate the math.\n",
        "2.  **Clustering:** The algorithm grouped products into 3 distinct clusters based on similarities."
      ],
      "metadata": {
        "id": "RIGaVLReECyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "products = {\n",
        "    'Product_ID': range(1, 11),\n",
        "    'Price': [20, 25, 1200, 1500, 10, 2800, 150, 45, 1300, 3000],\n",
        "    'Review_Score': [30, 40, 95, 88, 25, 99, 60, 45, 90, 98]\n",
        "}\n",
        "df_cluster = pd.DataFrame(products)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df_cluster[['Price', 'Review_Score']])\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "df_cluster['Cluster_Group'] = kmeans.fit_predict(scaled_data)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(data=df_cluster, x='Price', y='Review_Score', hue='Cluster_Group', palette='deep', s=100)\n",
        "plt.title('AI-Detected Product Clusters')\n",
        "plt.xlabel('Price ($)')\n",
        "plt.ylabel('Review Score (1-100)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D0JwHMQxHYiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“Š Project 4: Data Visualization\n",
        "\n",
        "### ðŸ“Œ Problem Statement\n",
        "Raw data in spreadsheets is difficult to interpret. The goal was to use **Data Visualization** to extract business insights regarding Seasonality and Category Performance.\n",
        "\n",
        "### âš™ï¸ Visualization Strategy\n",
        "* **Seaborn** was used for statistical styling.\n",
        "* **Matplotlib Subplots** allowed for a dashboard layout.\n",
        "* **Insights:** The visualizations revealed a strong correlation between the holiday season (Dec) and peak sales."
      ],
      "metadata": {
        "id": "qqNradw-ELix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_data = {\n",
        "    'Month': ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],\n",
        "    'Sales': [12000, 13000, 12500, 14000, 16000, 19000, 18000, 21000, 23000, 26000, 32000, 38000],\n",
        "    'Category': ['Electronics'] * 12\n",
        "}\n",
        "df_sales = pd.DataFrame(sales_data)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.lineplot(data=df_sales, x='Month', y='Sales', marker='o', linewidth=2.5, color='purple')\n",
        "plt.fill_between(df_sales['Month'], df_sales['Sales'], color='purple', alpha=0.1) # Shading area\n",
        "plt.title('Annual Sales Trend Analysis', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Revenue ($)')\n",
        "plt.grid(True, linestyle='--')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sZDB7i9FHp_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ—£ï¸ Project 5: Review Analysis (NLP)\n",
        "\n",
        "### ðŸ“Œ Problem Statement\n",
        "To further enhance review analysis, I built a Sentiment Analyzer using **Natural Language Processing (NLP)**.\n",
        "\n",
        "### âš™ï¸ Methodology\n",
        "Using the `TextBlob` library, the system assigns a **Polarity Score** to text:\n",
        "* **> 0:** Positive Sentiment\n",
        "* **< 0:** Negative Sentiment\n",
        "* **0:** Neutral"
      ],
      "metadata": {
        "id": "EBrp41A3ETvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = [\n",
        "    \"The customer service was extremely rude.\",\n",
        "    \"I am in love with the design!\",\n",
        "    \"It is just a normal product.\",\n",
        "    \"Highly recommended for everyone.\"\n",
        "]\n",
        "\n",
        "print(f\"{'REVIEW TEXT':<40} | {'SENTIMENT':<10} | {'SCORE'}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "for review in reviews:\n",
        "    analysis = TextBlob(review)\n",
        "    score = analysis.sentiment.polarity\n",
        "\n",
        "    if score > 0:\n",
        "        label = \"Positive ðŸŸ¢\"\n",
        "    elif score < 0:\n",
        "        label = \"Negative ðŸ”´\"\n",
        "    else:\n",
        "        label = \"Neutral ðŸŸ¡\"\n",
        "\n",
        "    print(f\"{review:<40} | {label:<10} | {score:.2f}\")"
      ],
      "metadata": {
        "id": "-eWmLdaqH6mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŽ“ Final Conclusion & Learning Outcomes\n",
        "\n",
        "This internship provided hands-on experience with the complete Data Science lifecycle. I learned that:\n",
        "1.  **Data Preprocessing** (Feature Engineering, Scaling) is often more important than the model itself.\n",
        "2.  **Unsupervised Learning** can find hidden patterns that humans miss.\n",
        "3.  **Visualization** is key to communicating results to stakeholders.\n",
        "\n",
        "I am now confident in using Python libraries to solve real-world Machine Learning problems."
      ],
      "metadata": {
        "id": "hOvUfspuogfi"
      }
    }
  ]
}